\chapter{Literature Review}
    \section{You Only Look Once: Unified, Real-Time Object Detection }
        In this paper\cite{redmon2016you}, Redmon et. al. presented a new approach to object detection. Authors compared their model with other Real-Time Systems namely Deformable Parts Model(DPM), R-CNN, Fast Detectors, Deep MultiBox, OverFeat, MultiGrasp. Authors took the best part from those models and build their own model which was faster, better and worked in real time. This model was fast as it only looked once in the entire image and then detect the object in the image as the regression problem. The base YOLO model processed images in real-time at 45 fps. A smaller version processed an astounding 155 fps still achieving double the mAP if the other real time detectrs.\\
        In this paper, the authors also included the architecture of the model, and the process the authors used for training. The final output of the network was the 7*7*30 tensor of predictions which could predict at most 98 object.

    \section{YOLO9000:Better, Faster, Stronger }
        Redmon et. al. presented about the YOLO v1 in \cite{redmon2016you}. YOLO model made signifant no. of localization error. It had very low recall. So, the authors presented a new model better than YOLO v1 and created YOLOv2 with the improvement in object localization and recall. In paper \cite{redmon2017yolo9000}, Authors not only presented about the YOLOv2 also about the YOLO9000 a realtime detection system. YOLO9000 was a realtime framework for detection more than 9000 object categories by jointly optimizing detection and classification. Authors used WordTree to combine data from various source and the authors had joint optimization technique to train simultaneously in ImageNet and COCO dataset. YOLO9000 was a strong step that closes the gap between the classification and detection.
        Authors made a better model by introducing Batch Normalization that significantly improved in convergence of model by eliminating regularization. Authors added BatchNormaliztion in all Convolutional Layer that increased mAP by 2\%. \\
        Authors created a model faster than ever by using darknet-19 as their base framework. Darknet-19 has 19 Convolutional layer and 5 max pooling layes with only 5.58 billion operation. This figure was less that the no. of operation on VGG-16. VGG-16 has 30.69 billion floating point operation for a single pass over a single image. So, it was obvious the YOLO to be faster.\\ 
        YOLO9000 was not only faster and better, but was stronger too. As, the authors purposed a mechanism for jointly training on classification data as well as detection data. The proposed method used image labelled for detection to learn detection-specific information like bounding box prediction, objectness score calculation and classify common objects. Also it used imagse with only class lables to expand the number of categories it can detect.
    \section{YOLOV3: An Incremental Model}
        Redmon et. al.\cite{redmon2018yolov3} presented some update to YOLO. Authors made the model little bigger than last time but more accurate. It was still fast and run in 22ms at 28.2 mAP for 320 * 320 image. It was as accurate as SSD but three times faster that SSD. Here, Authors did not use softmax activation function in last layer instead simply used independed logistic classifier. Also, During training Binary cross entropy was used for the class predicitions. This formulation helps when we move to more complex domain with many overlapping labels (i.e. woman and person).

    \section{YOLOv4: Optimal Speed and Accuracy of Object Detection}
        YOLOv4 is a one-stage object detection model that improves on YOLOv3 with several bags of tricks and modules introduced in the literature.\cite{bochkovskiy2020yolov4} In this section, the authors elaborated the details of YOLOv4. YOLO v4 was developed by three developers Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. YOLOv4 consists of: 
        \begin{itemize}
            \item Backbone: CSPDarknet53
            \item Neck: SPP , PAN
            \item Head: YOLOv3
        \end{itemize}
        YOLO v4 uses:

        \begin{itemize}
            \item Bag of Freebies (BoF) for backbone: CutMix andMosaic data augmentation, DropBlock regularization,Class label smoothing.
            \item Bag of Specials (BoS) for backbone: Mish activation, Cross-stage partial connections (CSP), Multiinput weighted residual connections (MiWRC).
            \item Bag of Freebies (BoF) for detector: CIoU-loss CmBN, DropBlock
            regularization, Mosaic data augmentation, Self-Adversarial Training, Eliminate grid sensitivity, Using multiple anchors for a single ground truth, Cosine annealing scheduler , Optimal hyperparameters, Random training shapes.
            \item Bag of Specials (BoS) for detector: Mish activation, SPP-block,
            SAM-block, PAN path-aggregation block, DIoU-NMS
        \end{itemize}
    \section{Deep-Sort/nwokje} 
        Previously a research paper for deepsort by Nicolai Wojke, Alex Bewley and Dietrich Paulus started integration appearance information to improve the performance of SORT.\cite{wojke2017simple} Due to this extension, the authors were able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework the authors placed much of the computational complexity into an offline pre-training stage where the authors learned a deep association metric on a large-scale person re-identification dataset. During online application, the authors established measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation showed that the extensions reduced the number of identity switches by 45\%, achieving overall competitive performance at high frame rates.
    \section{Deep SORT and YOLO v4 for people tracking and detection with Tensorflow backend/ LeonLok}
        In this project,YOLO v3 swapped out  for YOLO v4 and added the option for asynchronous processing, which significantly improved the FPS. However, FPS monitoring was disabled when asynchronous processing was used since it wasn't accurate. In addition, the algorithm from the paper of Deep-Sort by Nwokje was used for tracking of objects. The original method for confirming tracks was based simply on the number of times an object has been detected without considering detection confidence, leading to high tracking false positive rates when unreliable detections occur (i.e. low confidence true positives or high confidence false positives). The track filtering algorithm reduced that significantly by calculating the average detection confidence over a set number of detections before confirming a track. This code only detects and tracks people, but could be changed to detect other objects by changing lines so it could also be implemented for detecting different objects including person.

        \section{Maximum correntropy Kalman filter}
        Traditional Kalman filter (KF) was derived under the well-known minimum mean square error (MMSE) criterion, which was optimal under Gaussian assumption.\cite{chen2017maximum} The best part of the Kalman filter is that it is recursive, meaning where we take current readings, to predict the current state, then use the measurements and update our prediction.Traditional Kalman filter works well under Gaussian noises, but its performance may deteriorate significantly under non-Gaussian noises, especially when the underlying system is disturbed by impulsive noises. The main reason for this was that KF was developed based on the MMSE criterion, which captures only the second order statistics of the error signal and was sensitive to large outliers. To address this problem, the authors proposed in this work to use the MCC criterion to derive a new Kalmanfilter, which may perform much better in non-Gaussian noise environments, since correntropy contains second and higher order moments of the error. Following problem were solved:            
        \begin{itemize}
            \item When object was partially occluded, place weight or reliance on both motion and sensor measurement data on that object.
            \item if its fully occluded, shift a lot of weight on motion data of an object.
        \end{itemize}